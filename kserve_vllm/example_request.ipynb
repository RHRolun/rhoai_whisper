{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec840385",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q vllm vllm[audio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15769ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "from vllm.assets.audio import AudioAsset\n",
    "import numpy as np\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5645164f",
   "metadata": {},
   "source": [
    "### Example request with raw file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523edc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_endpoint = \"https://whisper-small-whisper.apps.dev.rhoai.rh-aiservices-bu.com\"\n",
    "infer_url = f\"{infer_endpoint}/v1/models/model:predict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db843be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio = AudioAsset(\"mary_had_lamb\").audio_and_sample_rate\n",
    "audio = librosa.load(\"test.wav\", sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ededa392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_test_audio(audio_and_sample_rate):\n",
    "    audio_array, sample_rate = audio_and_sample_rate\n",
    "    # Convert float32 numpy array to bytes\n",
    "    audio_bytes = audio_array.astype(np.float32).tobytes()\n",
    "    audio_b64 = base64.b64encode(audio_bytes).decode(\"utf-8\")\n",
    "    return (audio_b64, sample_rate)\n",
    "\n",
    "prompts = {\n",
    "    \"instances\": [\n",
    "        {\n",
    "            \"prompt\": \"<|startoftranscript|>\",\n",
    "            \"multi_modal_data\": {\n",
    "                \"audio\": serialize_test_audio(audio),\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df512cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(infer_url, json=prompts, verify=False)\n",
    "response_dict = response.json()\n",
    "response_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74185699",
   "metadata": {},
   "source": [
    "### Example request with filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84fc88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_endpoint = \"https://whisper-small-whisper.apps.dev.rhoai.rh-aiservices-bu.com\"\n",
    "infer_url = f\"{infer_endpoint}/v1/models/model:predict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b1f61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "prompts = {\n",
    "    \"instances\": [\"test.wav\"]\n",
    "}\n",
    "\n",
    "response = requests.post(infer_url, json=prompts, verify=False)\n",
    "response_dict = response.json()\n",
    "response_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
