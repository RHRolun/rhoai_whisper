{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fbc864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Read and encode audio file\n",
    "with open(\"test.wav\", \"rb\") as f:\n",
    "    audio_bytes = f.read()\n",
    "audio_base64 = base64.b64encode(audio_bytes).decode(\"utf-8\")\n",
    "\n",
    "# Create the multi_modal_data JSON *as a string*\n",
    "multi_modal_data = {\n",
    "    \"audio\": {\n",
    "        \"data\": audio_base64,\n",
    "        \"sample_rate\": 16000\n",
    "    }\n",
    "}\n",
    "multi_modal_data_str = json.dumps(multi_modal_data)\n",
    "\n",
    "# Construct final payload\n",
    "payload = {\n",
    "    \"text_input\": \"<|startoftranscript|>\",\n",
    "    \"multi_modal_data\": multi_modal_data_str,\n",
    "}\n",
    "\n",
    "# Send request\n",
    "response = requests.post(\n",
    "    \"http://whisper-triton-vllm-predictor.whisper.svc.cluster.local:8000/v2/models/whisper/generate\",\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    "    data=json.dumps(payload)\n",
    ")\n",
    "\n",
    "# Print response\n",
    "print(\"Status:\", response.status_code)\n",
    "print(\"Response:\", response.text)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
